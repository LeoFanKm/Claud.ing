# ç”Ÿäº§ç¯å¢ƒæ£€æŸ¥æ¸…å•

> **ç›®æ ‡**ï¼šç¡®ä¿ Context Engineering ç³»ç»Ÿåœ¨ç”Ÿäº§ç¯å¢ƒä¸­å®‰å…¨ã€é«˜æ€§èƒ½ã€å¯æ‰©å±•

## ğŸ” å®‰å…¨æ€§æ£€æŸ¥æ¸…å•

### 1. PIIï¼ˆä¸ªäººæ•æ„Ÿä¿¡æ¯ï¼‰å¤„ç†

#### âœ… å¿…é¡»å®ç°

- [ ] **PII æ£€æµ‹å’Œè„±æ•**
  ```python
  def redact_pii(text):
      # é‚®ç®±
      text = re.sub(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', '[EMAIL]', text)
      # ç”µè¯
      text = re.sub(r'\b\d{3}[-.]?\d{4}[-.]?\d{4}\b', '[PHONE]', text)
      # èº«ä»½è¯å·ï¼ˆä¸­å›½ï¼‰
      text = re.sub(r'\b\d{17}[\dXx]\b', '[ID_NUMBER]', text)
      return text
  ```

- [ ] **å­˜å‚¨å‰è‡ªåŠ¨è„±æ•**
  ```python
  def store_memory(memory):
      memory["content"] = redact_pii(memory["content"])
      db.save(memory)
  ```

- [ ] **æ—¥å¿—ä¸­ä¸è®°å½• PII**
  ```python
  # âŒ é”™è¯¯
  logger.info(f"User {user_email} logged in")

  # âœ… æ­£ç¡®
  logger.info(f"User {hash(user_email)} logged in")
  ```

#### ğŸ“‹ åˆè§„è¦æ±‚

| æ³•è§„ | è¦æ±‚ | æ£€æŸ¥é¡¹ |
|-----|------|--------|
| **GDPR**ï¼ˆæ¬§ç›Ÿï¼‰ | æ•°æ®åˆ é™¤æƒ | å®ç° `delete_user_data()` API |
| **CCPA**ï¼ˆåŠ å·ï¼‰ | æ•°æ®è®¿é—®æƒ | å®ç° `export_user_data()` API |
| **PIPL**ï¼ˆä¸­å›½ï¼‰ | æ•°æ®æœ¬åœ°åŒ– | ä¸­å›½ç”¨æˆ·æ•°æ®å­˜å‚¨åœ¨ä¸­å›½ |

### 2. æ•°æ®éš”ç¦»

#### âœ… å¿…é¡»å®ç°

- [ ] **ç”¨æˆ·çº§åˆ«éš”ç¦»**
  ```python
  # æ‰€æœ‰æŸ¥è¯¢å¿…é¡»åŒ…å« user_id è¿‡æ»¤
  def get_memories(user_id):
      return db.query("SELECT * FROM memories WHERE user_id = ?", user_id)
  ```

- [ ] **Multi-tenancyï¼ˆå¤šç§Ÿæˆ·ï¼‰éš”ç¦»**
  ```python
  # å¦‚æœæ”¯æŒä¼ä¸šå®¢æˆ·ï¼Œå¿…é¡»éš”ç¦»ç§Ÿæˆ·æ•°æ®
  def get_memories(tenant_id, user_id):
      return db.query(
          "SELECT * FROM memories WHERE tenant_id = ? AND user_id = ?",
          tenant_id, user_id
      )
  ```

- [ ] **è®¿é—®æ§åˆ¶éªŒè¯**
  ```python
  def can_access_memory(user_id, memory_id):
      memory = db.get_memory(memory_id)
      return memory["user_id"] == user_id
  ```

### 3. Memory é˜²æŠ•æ¯’

#### âœ… å¿…é¡»å®ç°

- [ ] **è¾“å…¥éªŒè¯**
  ```python
  def validate_memory_input(content):
      # é•¿åº¦é™åˆ¶
      if len(content) > 10000:
          raise ValueError("Content too long")

      # æ¶æ„æ¨¡å¼æ£€æµ‹
      malicious_patterns = [
          r"ignore previous instructions",
          r"system:\s*you are now",
          r"<script>",
          r"eval\(",
      ]

      for pattern in malicious_patterns:
          if re.search(pattern, content, re.IGNORECASE):
              raise ValueError("Potentially malicious content")

      return True
  ```

- [ ] **Provenance éªŒè¯**
  ```python
  # æ¯æ¡è®°å¿†å¿…é¡»æœ‰æ¥æºè¿½è¸ª
  required_provenance_fields = [
      "source_type",
      "timestamp",
      "confidence",
      "session_id"
  ]

  for field in required_provenance_fields:
      if field not in memory["provenance"]:
          raise ValueError(f"Missing provenance field: {field}")
  ```

- [ ] **å†…å®¹å®¡æ ¸**
  ```python
  # ä½¿ç”¨ AI å®¡æ ¸ï¼ˆå¯é€‰ä½†æ¨èï¼‰
  def moderate_content(content):
      result = moderation_api.check(content)
      if result["flagged"]:
          raise ValueError("Content violates policy")
      return True
  ```

## âš¡ æ€§èƒ½æ£€æŸ¥æ¸…å•

### 1. å»¶è¿Ÿç›®æ ‡

| æ“ä½œ | P50 | P95 | P99 |
|-----|-----|-----|-----|
| **Session åˆ›å»º** | <50ms | <100ms | <200ms |
| **Event è¿½åŠ ** | <20ms | <50ms | <100ms |
| **Memory æ£€ç´¢** | <100ms | <200ms | <500ms |
| **Memory ç”Ÿæˆ** | å¼‚æ­¥ | å¼‚æ­¥ | å¼‚æ­¥ |

#### âœ… å¿…é¡»å®ç°

- [ ] **å¼‚æ­¥ Memory ç”Ÿæˆ**
  ```python
  @app.post("/chat")
  async def chat(request):
      # åŒæ­¥ï¼šè¿”å›å“åº”
      response = generate_response(request)

      # å¼‚æ­¥ï¼šåå°ç”Ÿæˆè®°å¿†ï¼ˆä¸é˜»å¡ï¼‰
      asyncio.create_task(generate_memories(request.session_id))

      return response
  ```

- [ ] **ç¼“å­˜çƒ­é—¨æ•°æ®**
  ```python
  from functools import lru_cache

  @lru_cache(maxsize=1000)
  def get_user_profile(user_id):
      return db.query("SELECT * FROM user_profiles WHERE user_id = ?", user_id)
  ```

- [ ] **æ‰¹é‡æ“ä½œ**
  ```python
  # âŒ é”™è¯¯ï¼šN æ¬¡æ•°æ®åº“è°ƒç”¨
  for memory in memories:
      db.insert(memory)

  # âœ… æ­£ç¡®ï¼š1 æ¬¡æ‰¹é‡æ’å…¥
  db.bulk_insert(memories)
  ```

### 2. Token ä¼˜åŒ–

#### âœ… å¿…é¡»å®ç°

- [ ] **ä¸Šä¸‹æ–‡çª—å£ç›‘æ§**
  ```python
  def estimate_tokens(events):
      # ç®€å•ä¼°ç®—ï¼š4 å­—ç¬¦ â‰ˆ 1 token
      text = json.dumps(events)
      return len(text) // 4

  def check_context_size(session):
      tokens = estimate_tokens(session["events"])
      max_tokens = 128000  # ä¾‹å¦‚ï¼šGemini 2.0 Pro

      if tokens > max_tokens * 0.8:
          logger.warning(f"High token usage: {tokens}/{max_tokens}")
          compress_session(session)
  ```

- [ ] **æ™ºèƒ½å‹ç¼©ç­–ç•¥**
  ```python
  def compress_session(session):
      # ä¿ç•™æœ€è¿‘ 20 è½®å¯¹è¯
      recent = session["events"][-20:]

      # ä¿ç•™é‡è¦äº‹ä»¶
      important = [
          e for e in session["events"][:-20]
          if e.get("important", False)
      ]

      # æ‘˜è¦å…¶ä»–äº‹ä»¶
      old_events = [
          e for e in session["events"][:-20]
          if not e.get("important", False)
      ]
      summary = generate_summary(old_events)

      session["events"] = [
          {"type": "summary", "content": summary}
      ] + important + recent
  ```

### 3. æ•°æ®åº“ä¼˜åŒ–

#### âœ… å¿…é¡»å®ç°

- [ ] **ç´¢å¼•ä¼˜åŒ–**
  ```sql
  -- å¿…é¡»åˆ›å»ºçš„ç´¢å¼•
  CREATE INDEX idx_memories_user_id ON memories(user_id);
  CREATE INDEX idx_memories_timestamp ON memories(timestamp);
  CREATE INDEX idx_sessions_user_id ON sessions(user_id);
  CREATE INDEX idx_sessions_created_at ON sessions(created_at);
  ```

- [ ] **æŸ¥è¯¢ä¼˜åŒ–**
  ```python
  # âŒ é”™è¯¯ï¼šSELECT *
  db.query("SELECT * FROM memories WHERE user_id = ?", user_id)

  # âœ… æ­£ç¡®ï¼šåªæŸ¥è¯¢éœ€è¦çš„å­—æ®µ
  db.query("SELECT id, content, type FROM memories WHERE user_id = ?", user_id)
  ```

- [ ] **è¿æ¥æ± é…ç½®**
  ```python
  # æ•°æ®åº“è¿æ¥æ± 
  db_pool = create_pool(
      min_size=10,
      max_size=100,
      timeout=30
  )
  ```

## ğŸ“ˆ å¯æ‰©å±•æ€§æ£€æŸ¥æ¸…å•

### 1. æ¶æ„æ¨¡å¼

#### âœ… å¿…é¡»å®ç°

- [ ] **æ— çŠ¶æ€æœåŠ¡**
  ```python
  # âŒ é”™è¯¯ï¼šå…¨å±€å˜é‡å­˜å‚¨çŠ¶æ€
  user_sessions = {}

  # âœ… æ­£ç¡®ï¼šå­˜å‚¨åœ¨æ•°æ®åº“/ç¼“å­˜
  def get_session(session_id):
      return redis.get(f"session:{session_id}")
  ```

- [ ] **æ°´å¹³æ‰©å±•æ”¯æŒ**
  ```python
  # ä½¿ç”¨è´Ÿè½½å‡è¡¡å™¨ï¼ˆå¦‚ï¼šNginxï¼‰åˆ†å‘è¯·æ±‚
  # å¤šä¸ªæœåŠ¡å®ä¾‹å…±äº«åŒä¸€ä¸ªæ•°æ®åº“å’Œç¼“å­˜

  # é…ç½®ç¤ºä¾‹ï¼ˆdocker-composeï¼‰
  services:
    app:
      image: my-app:latest
      replicas: 3  # 3 ä¸ªå®ä¾‹
      environment:
        - DB_HOST=postgres
        - REDIS_HOST=redis
  ```

- [ ] **æ•°æ®åˆ†ç‰‡ï¼ˆShardingï¼‰**
  ```python
  def get_shard_id(user_id, num_shards=10):
      return hash(user_id) % num_shards

  def get_user_db(user_id):
      shard_id = get_shard_id(user_id)
      return db_connections[shard_id]
  ```

### 2. å®¹é‡è§„åˆ’

#### ğŸ“Š ä¼°ç®—æŒ‡æ ‡

| æŒ‡æ ‡ | å°è§„æ¨¡ | ä¸­è§„æ¨¡ | å¤§è§„æ¨¡ |
|-----|--------|--------|--------|
| **ç”¨æˆ·æ•°** | <10K | 10K-100K | >100K |
| **æ¯ç”¨æˆ·è®°å¿†** | <100 | 100-1K | >1K |
| **æ¯æ—¥å¯¹è¯** | <10K | 10K-100K | >100K |
| **å­˜å‚¨éœ€æ±‚** | <1GB | 1-100GB | >100GB |

#### âœ… å¿…é¡»å®ç°

- [ ] **æ•°æ®æ¸…ç†ç­–ç•¥**
  ```python
  # å®šæœŸæ¸…ç†æ—§ Sessionï¼ˆä¾‹å¦‚ï¼š7 å¤©å‰ï¼‰
  def cleanup_old_sessions(days=7):
      cutoff = datetime.now() - timedelta(days=days)
      db.execute(
          "DELETE FROM sessions WHERE created_at < ?",
          cutoff
      )

  # å®šæœŸæ¸…ç†è¿‡æ—¶ Memoryï¼ˆä¾‹å¦‚ï¼š90 å¤©æœªè®¿é—®ï¼‰
  def cleanup_stale_memories(days=90):
      cutoff = datetime.now() - timedelta(days=days)
      db.execute(
          "DELETE FROM memories WHERE last_accessed < ? AND important = false",
          cutoff
      )
  ```

- [ ] **å½’æ¡£ç­–ç•¥**
  ```python
  # å½’æ¡£æ—§æ•°æ®åˆ°å†·å­˜å‚¨ï¼ˆå¦‚ï¼šS3ï¼‰
  def archive_old_data(days=180):
      cutoff = datetime.now() - timedelta(days=days)
      old_data = db.query(
          "SELECT * FROM memories WHERE created_at < ?",
          cutoff
      )

      # ä¸Šä¼ åˆ° S3
      s3.put_object(
          Bucket="archived-memories",
          Key=f"archive_{cutoff.isoformat()}.json",
          Body=json.dumps(old_data)
      )

      # ä»æ•°æ®åº“åˆ é™¤
      db.execute("DELETE FROM memories WHERE created_at < ?", cutoff)
  ```

### 3. ç›‘æ§å’Œå‘Šè­¦

#### âœ… å¿…é¡»å®ç°

- [ ] **å…³é”®æŒ‡æ ‡ç›‘æ§**
  ```python
  from prometheus_client import Counter, Histogram

  # è¯·æ±‚è®¡æ•°
  request_counter = Counter('memory_requests_total', 'Total memory requests')

  # å»¶è¿Ÿåˆ†å¸ƒ
  latency_histogram = Histogram('memory_latency_seconds', 'Memory operation latency')

  # é”™è¯¯ç‡
  error_counter = Counter('memory_errors_total', 'Total memory errors')

  @latency_histogram.time()
  def retrieve_memories(user_id):
      try:
          request_counter.inc()
          memories = db.query("SELECT * FROM memories WHERE user_id = ?", user_id)
          return memories
      except Exception as e:
          error_counter.inc()
          raise
  ```

- [ ] **å‘Šè­¦è§„åˆ™**
  ```yaml
  # Prometheus å‘Šè­¦è§„åˆ™ç¤ºä¾‹
  groups:
    - name: memory_system
      rules:
        # é”™è¯¯ç‡ > 5%
        - alert: HighErrorRate
          expr: rate(memory_errors_total[5m]) / rate(memory_requests_total[5m]) > 0.05
          for: 5m
          annotations:
            summary: "High error rate in memory system"

        # P95 å»¶è¿Ÿ > 500ms
        - alert: HighLatency
          expr: histogram_quantile(0.95, memory_latency_seconds) > 0.5
          for: 5m
          annotations:
            summary: "High P95 latency in memory operations"
  ```

## ğŸ§ª æµ‹è¯•æ£€æŸ¥æ¸…å•

### 1. å•å…ƒæµ‹è¯•

#### âœ… å¿…é¡»è¦†ç›–

- [ ] **Memory æå–é€»è¾‘**
  ```python
  def test_memory_extraction():
      events = [
          {"content": "I prefer concise answers"},
          {"content": "My name is Alice"}
      ]

      memories = extract_memories(events)

      assert len(memories) == 2
      assert memories[0]["type"] == "preference"
      assert memories[1]["type"] == "personal_info"
  ```

- [ ] **Memory æ•´åˆé€»è¾‘**
  ```python
  def test_merge_duplicate_memories():
      memories = [
          {"content": "User likes concise answers", "timestamp": "2025-01-01"},
          {"content": "User prefers brief responses", "timestamp": "2025-01-02"}
      ]

      merged = merge_duplicate_memories(memories)

      assert len(merged) == 1  # åˆå¹¶é‡å¤
      assert merged[0]["timestamp"] == "2025-01-02"  # ä¿ç•™æœ€æ–°
  ```

- [ ] **PII è„±æ•**
  ```python
  def test_pii_redaction():
      text = "My email is test@example.com and phone is 138-0000-0000"
      redacted = redact_pii(text)

      assert "test@example.com" not in redacted
      assert "138-0000-0000" not in redacted
      assert "[EMAIL]" in redacted
      assert "[PHONE]" in redacted
  ```

### 2. é›†æˆæµ‹è¯•

#### âœ… å¿…é¡»è¦†ç›–

- [ ] **ç«¯åˆ°ç«¯ Session æµç¨‹**
  ```python
  def test_session_lifecycle():
      # åˆ›å»º Session
      session_id = create_session(user_id="user_123")

      # æ·»åŠ äº‹ä»¶
      add_event(session_id, {"type": "user_input", "content": "Hello"})
      add_event(session_id, {"type": "agent_response", "content": "Hi there!"})

      # æ£€ç´¢ Session
      session = get_session(session_id)
      assert len(session["events"]) == 2

      # å‹ç¼© Session
      compress_session(session)
      assert len(session["events"]) < 2  # å·²å‹ç¼©
  ```

- [ ] **Memory ç”Ÿæˆå’Œæ£€ç´¢**
  ```python
  def test_memory_generation_and_retrieval():
      # ç”Ÿæˆ Memory
      session_id = create_session_with_events(user_id="user_123")
      generate_memories(session_id)

      # æ£€ç´¢ Memory
      memories = retrieve_memories("user_123", query="user preferences")
      assert len(memories) > 0
      assert "preference" in memories[0]["type"]
  ```

### 3. æ€§èƒ½æµ‹è¯•

#### âœ… å¿…é¡»è¦†ç›–

- [ ] **è´Ÿè½½æµ‹è¯•**
  ```python
  import time
  from concurrent.futures import ThreadPoolExecutor

  def test_load():
      # æ¨¡æ‹Ÿ 100 ä¸ªå¹¶å‘ç”¨æˆ·
      with ThreadPoolExecutor(max_workers=100) as executor:
          futures = [
              executor.submit(retrieve_memories, f"user_{i}")
              for i in range(100)
          ]

          # ç­‰å¾…æ‰€æœ‰è¯·æ±‚å®Œæˆ
          results = [f.result() for f in futures]

      # éªŒè¯å»¶è¿Ÿ
      assert all(r["latency"] < 500 for r in results)  # P99 < 500ms
  ```

- [ ] **å‹åŠ›æµ‹è¯•**
  ```python
  def test_stress():
      # æŒç»­ 1 åˆ†é’Ÿï¼Œæ¯ç§’ 1000 è¯·æ±‚
      duration = 60
      rps = 1000

      start_time = time.time()
      request_count = 0
      error_count = 0

      while time.time() - start_time < duration:
          try:
              retrieve_memories(f"user_{request_count % 100}")
              request_count += 1
          except Exception:
              error_count += 1

          time.sleep(1 / rps)

      # é”™è¯¯ç‡ < 1%
      assert error_count / request_count < 0.01
  ```

## ğŸ“‹ éƒ¨ç½²å‰æ£€æŸ¥æ¸…å•

### âœ… å®‰å…¨æ€§

- [ ] PII è„±æ•æœºåˆ¶å·²å®ç°
- [ ] æ•°æ®éš”ç¦»å·²éªŒè¯
- [ ] Memory é˜²æŠ•æ¯’å·²å®ç°
- [ ] è®¿é—®æ§åˆ¶å·²æµ‹è¯•
- [ ] åˆè§„è¦æ±‚å·²æ»¡è¶³ï¼ˆGDPR/CCPA/PIPLï¼‰

### âœ… æ€§èƒ½

- [ ] å¼‚æ­¥ Memory ç”Ÿæˆå·²å®ç°
- [ ] ç¼“å­˜ç­–ç•¥å·²é…ç½®
- [ ] æ•°æ®åº“ç´¢å¼•å·²åˆ›å»º
- [ ] Token ä¼˜åŒ–å·²å®ç°
- [ ] å»¶è¿Ÿç›®æ ‡å·²éªŒè¯ï¼ˆP50/P95/P99ï¼‰

### âœ… å¯æ‰©å±•æ€§

- [ ] æ— çŠ¶æ€æœåŠ¡æ¶æ„
- [ ] æ°´å¹³æ‰©å±•å·²æµ‹è¯•
- [ ] æ•°æ®æ¸…ç†ç­–ç•¥å·²å®ç°
- [ ] å®¹é‡è§„åˆ’å·²å®Œæˆ
- [ ] ç›‘æ§å’Œå‘Šè­¦å·²é…ç½®

### âœ… æµ‹è¯•

- [ ] å•å…ƒæµ‹è¯•è¦†ç›–ç‡ > 80%
- [ ] é›†æˆæµ‹è¯•å·²é€šè¿‡
- [ ] è´Ÿè½½æµ‹è¯•å·²é€šè¿‡
- [ ] å‹åŠ›æµ‹è¯•å·²é€šè¿‡

### âœ… è¿ç»´

- [ ] æ—¥å¿—è®°å½•å·²é…ç½®
- [ ] ç›‘æ§æŒ‡æ ‡å·²æš´éœ²
- [ ] å‘Šè­¦è§„åˆ™å·²è®¾ç½®
- [ ] å¤‡ä»½ç­–ç•¥å·²å®ç°
- [ ] å›æ»šè®¡åˆ’å·²å‡†å¤‡

## ğŸ”— å»¶ä¼¸é˜…è¯»

- **[Sessions è¯¦ç»†æŒ‡å—](./sessions-guide.md)** - Session ç®¡ç†æœ€ä½³å®è·µ
- **[Memory è¯¦ç»†æŒ‡å—](./memory-guide.md)** - Memory ç³»ç»Ÿè®¾è®¡
- **[ä¸» Skill æ–‡æ¡£](./skill.md)** - Context Engineering æ ¸å¿ƒæ¦‚å¿µ

---

**å…³é”®è¦ç‚¹**ï¼š
- å®‰å…¨ä¼˜å…ˆï¼šPIIã€æ•°æ®éš”ç¦»ã€é˜²æŠ•æ¯’
- æ€§èƒ½ç›®æ ‡ï¼šP50 <100msã€P95 <200msã€P99 <500ms
- å¯æ‰©å±•ï¼šæ— çŠ¶æ€ã€æ°´å¹³æ‰©å±•ã€æ•°æ®åˆ†ç‰‡
- ç›‘æ§å‘Šè­¦ï¼šå…³é”®æŒ‡æ ‡ã€å‘Šè­¦è§„åˆ™ã€æ—¥å¿—è®°å½•
- å……åˆ†æµ‹è¯•ï¼šå•å…ƒã€é›†æˆã€è´Ÿè½½ã€å‹åŠ›æµ‹è¯•
